estimate_params from this link needs to be updated: https://nasa.github.io/progpy/api_ref/prog_models/PrognosticModel.html#prog_models.PrognosticsModel.estimate_params
, updated in the function definition, however not on the website.

 - Created test to see what happens if bounds go the wrong way. i.e (30, 7) instead of (7, 30)

Think of ways to create more descriptive error messages

Begin checking other parameters for params_estimate

Statement ____ Swap ordering in params_estimate for bounds and method. ^^website

Question*** Shouldn't changing optimization method chagne end values?

Are default options set to make it various questions more optimal

Idea: on line 1238 of pringistcs_model, I provide change the logic to provide a more acurate error message and detail what exactly is not equal to one another.
            and potneitally have their values shown.

Question*** Is runs 'depreciated' since one of the parameters is marked as 'depreciated'

        # with self.assertRaises(ValueError):
        # Does not do anything. Probably want to include an error message or some indicator to let the user know
        # in case user is expected change however is not seeing it.

Line 148 does not work as intended?

Talk about runs parameter.

        # Consider giving a warning LINE121 estimate_params

#1240 

                # not in for loops perferably, runtime

# tolerance parameter in minimize function - (tol)

# larger tolerance = larger error


        # If not in a list, and only one main data passed in, then you want to run without failing, additional feature needed


# Review simulate_to_threshold and make sure previous tests are working as intended


        simulated_results = m.simulate_to(2000, future_loading, **options)


        value = m.calc_error(simulated_results.times, simulated_results.inputs, simulated_results.outputs, dt=2)

        m.parameters['alpha'] = 3000
        keys = ['alpha']
        
        error = m.calc_error(simulated_results.times, simulated_results.inputs, simulated_results.outputs, dt=2)
        
        # error returns 0


# talk about the different ways runs is created



test_estimate_params 
# Should be passing.
m.estimate_params(times=[[times]], inputs=[[inputs]], outputs=[[outputs]])
        # This will work after future change 

prognostics_model
                # try:
                #     outputs = [self.OutputContainer(u_i) for u_i in outputs]
                # except:
                #     raise TypeError("Passing in too many lists aorund outputs")
        
1307, if not isinstance(inputs[0], self.InputContainer):

# Also keeping track of which run specifcally is failing.
        - run 0 vs run 1
        - Note about specific implementation. Even if the user does not pass in a particular 'run' persay, the error message will
still communicate that they are failing on the first run. Easy fix if it is worth the effort.

separate pull request for tol feature in params_estimate.
# Move all changes here:

        
        # Testing if tolerance works
        m.estimate_params(data, keys, bounds=bound, method='Powell', tol=2.00)
        m1.estimate_params(data, keys, bounds=bound, method='Powell', tol=1e-6)

        self.assertNotAlmostEqual(m.calc_error(times, inputs, outputs), m1.calc_error(times, inputs, outputs))


        m.estimate_params(data, keys, bounds=bound, method='Powell', tol=1e-6)
        m1.estimate_params(data, keys, bounds=bound, method='Powell', tol=1e-6)

        self.assertAlmostEqual(m.calc_error(times, inputs, outputs), m1.calc_error(times, inputs, outputs), 2)

        res = minimize(optimization_fcn, params, method=config['method'], bounds = config['bounds'], options=config['options'], tol=config['tol'])

        config = {
            'method': 'nelder-mead',
            'bounds': tuple((-np.inf, np.inf) for _ in keys),
            'options': {'xatol': 1e-8},
            'tol': None
        }

Add some extra documentation comments as well.


Complete Git Issue #504 in a separate pull request.
        PRS:
                params_estimate testing and additional checks
                tol
                division/zero
                calc_error testing and additional checks

                composite_model

**kwargs Consider adding some testing for calc_error. Should only be flagged when calling calc_error by itself.

THRESHOLD

Ask about try except blocks
Inputting other types in correct format works. Devalidates a few other tests prior since they are not really testing typing

        # when bounds are the same, should we give a warning, same as not changing the keys at all
        with self.assertRaises(ValueError):
            m.estimate_params(data, keys, bounds=[(0, 0), (1, 4), (-20, 20)])

        # Should be provide a warning when using a set within a set?
        m.estimate_params(data, keys, bounds={{-4, 4}, {-2, 0}, {0, 8}})


        m.estimate_params(data, keys, bounds=[{0, 4}, {20, 42}, {-4, 15}])

        # Lower Boung greater than Upper BOund
        with self.assertRaises(ValueError):
            m.estimate_params(data, keys, bounds=[[4, 0], [-20, 20], [0, 40]])

        # Consider creating a TypeError
        # # with self.assertRaises(ValueError):
        # #     # bounds isn't tuple or dict
        # m.estimate_params(data, keys, bounds='a')

        # Throwing a warning to the user to state which bounds is not matching a value of the keys, thus not used in estimateparams
        m.estimate_params(data, keys, bounds={'thrower_height': (0, 4), 'throwing_speed': (20, 42), 'g': (-20, 0), 'rho': (-100, 100)})
        for key in keys:
            self.assertAlmostEqual(m.parameters[key], gt[key], 2)


        # Throws a type error if bounds are Sets. Has to be specific enough to mention that Bounds cannot be undefined ordered Sequence?
        m2.estimate_params(data, keys, bounds=[{0, 5}, {42, 20}, {-4, 16}])

        m.estimate_params(data, keys, bounds=[{0, 4}, {20, 42}, {-4, 15}])

        m.estimate_params(data, keys, bounds={{0, 4}, {20, 42}, {-4, 15}})

Using sets should not be a thing due to "unhashable" type errors

Talk about bounds that are 'equal'


        # self.assertAlmostEqual(value1, value2)

        # interesting behavior where it states the lower bounds is greater than an upper bounds

        # Throws a type error if bounds are Sets. Has to be specific enough to mention that Bounds cannot be undefined ordered Sequence?

        # m.estimate_params(data, keys, bounds=[{0, 4}, {20, 42}, {-4, 15}])

        # m.estimate_params(data, keys, bounds={{0, 4}, {20, 42}, {-4, 15}})

        # This is a Pythonic Error, unhashable, cannot have sets within sets
        # m.estimate_params(data, keys, bounds={{0, 4}, {20, 42}, {-4, 15}})


# Note about estimate_params. Calling it on extremely big values and/or having bounds be very obviously incorrect will
# result in a incorrectly defined behavior. I can make examples detailing common examples of this occuring would be using  
# the battery and having some parameters be VERY wrong. Also need to look into making other parameters here very wrong.
# For the latter case, would this confirm that it would be possible for a chain of estimate_params to be called? Makes sense
# however, a bit interesting... After giving one bounds, possible for other bounds to change? With same bounds does not happen,
# with different bounds occurs? How are we supposed to clearly keep track of this behavior...


        # Throw warning:
                - Lower and Upper bounds cannot be the same value.
        m.estimate_params(data, keys, bounds=[np.array(['9', '9']), np.array(['2', '3']), np.array(['4', '5'])])

        m.estimate_params(data, keys, bounds=((1, 1), (2, 4), (-1, 24)))

# Keys that are not defined in specs
# Should this error out, maybe a warning should be provided for all the args that do not exist?
m.estimate_params(data, keys, bounds=bound, method='TNC', options={'1':2, '2':2, '3':3})




Q - str to x format stays consistent with future releases? 

        # Not setting up 'maxiter' and/or 'disp'
        # Needs to be str: int format.
        with self.assertRaises(TypeError):
            m.estimate_params(data, keys, bounds=bound, method='Powell', options= {1:2, True:False})
        with self.assertRaises(TypeError):
            m.estimate_params(data, keys, bounds=bound, method='Powell', options={'maxiter': '3', 'disp': False})

# NOTE
# m.estimate_params(data, keys, bounds=((1.243, 4.0), (19.13212, 39.12301), (-21.123, 2.000991)))
# check2= m.calc_error(results.times, results.inputs, results.outputs)

# Note about estimate_params. Calling it on extremely big values and/or having bounds be very obviously incorrect will
# result in a incorrectly defined behavior. I can make examples detailing common examples of this occuring would be using  
# the battery and having some parameters be VERY wrong. Also need to look into making other parameters here very wrong.
# For the latter case, would this confirm that it would be possible for a chain of estimate_params to be called? Makes sense
# however, a bit interesting... After giving one bounds, possible for other bounds to change? With same bounds does not happen,
# with different bounds occurs? How are we supposed to clearly keep track of this behavior...

# calc_error is a much bigger value compared to other values. Not including a good range?
# Should not affect other calls...



        # gt copy works as intended. Going to a different object location

        # Create a model that will easily raise an Exception with calc_error.





















    @unittest.skip
    def test_big_example(self):
        # Note, lowering timesteps or increasing simulate threshold may cause this model to not run (takes too long)
        m = BatteryElectroChemEOD()

        options = {
            'save_freq': 200, # Frequency at which results are saved
            'dt': 1, # Timestep
        }

        def future_loading(t, x=None):
            if (t < 600):
                i = 2
            elif (t < 900):
                i = 1
            elif (t < 1800):
                i = 4
            elif (t < 3000):
                i = 2
            else:
                i = 3
            return m.InputContainer({'i': i})
    
        simulated_results = m.simulate_to(1200, future_loading, **options)

        value = m.calc_error(simulated_results.times, simulated_results.inputs, simulated_results.outputs, dt=1)

        # Creating errors
        m.parameters['qMax'] = 12000
        keys = ['qMax']

        error = m.calc_error(simulated_results.times, simulated_results.inputs, simulated_results.outputs, dt=1)

        m.estimate_params([(simulated_results.times, simulated_results.inputs, simulated_results.outputs)], keys, dt=0.5)

        value1 = m.calc_error(simulated_results.times, simulated_results.inputs, simulated_results.outputs, dt=1)
        
        # After estimating parameters, value1 needs to at least be less than error
        self.assertLessEqual(value1, error)

        m1 = BatteryElectroChemEOD()

        m.parameters['kp'] = m1.parameters['kp'] = 10000
        # m.parameters['vMax'] = m1.parameters['vMax'] = 3000
        m.parameters['kn'] = m1.parameters['kn'] = 1000 #Does not error out even though value is unbelivably off
        m.parameters['qpMax'] = m1.parameters['qpMax'] = 4500
        m.parameters['qMax'] = m1.parameters['qMax'] = 9000
        keys = ['kp', 'kn', 'qpMax', 'qMax']

        simulated_results = m.simulate_to(2000, future_loading, **options)
        m1_sim_results = m1.simulate_to(2000, future_loading, **options)

        data = [(simulated_results.times, simulated_results.inputs, simulated_results.outputs)]
        data_m1 = [(m1_sim_results.times, m1_sim_results.inputs, m1_sim_results.outputs)]

        # Check out the warnings that are occuring here...
        # They are being spammed almost. Increasing save_frequency increases spam
        m.estimate_params(data, keys, method='Powell')
        m1.estimate_params(data_m1, keys, method='CG')

        self.assertNotAlmostEqual(m.calc_error(simulated_results.times, simulated_results.inputs, simulated_results.outputs, dt = 1), 
                                  m1.calc_error(m1_sim_results.times, m1_sim_results.inputs, m1_sim_results.outputs, dt = 1))

        m.estimate_params(data, keys, method='Powell', options={'maxiter': 1e-9, 'disp': False})
        m1.estimate_params(data_m1, keys, method='CG', options={'maxiter': 1e-9, 'disp': False})

        self.assertNotAlmostEqual(m.calc_error(simulated_results.times, simulated_results.inputs, simulated_results.outputs, dt = 1), 
                                  m1.calc_error(m1_sim_results.times, m1_sim_results.inputs, m1_sim_results.outputs, dt = 1))
        
        m.estimate_params(data, keys, method='Powell', options={'maxiter': 2, 'disp': False})
        m1.estimate_params(data_m1, keys, method='CG', options={'maxiter': 1e-9, 'disp': False})

        self.assertNotAlmostEqual(m.calc_error(simulated_results.times, simulated_results.inputs, simulated_results.outputs, dt = 1), 
                                  m1.calc_error(m1_sim_results.times, m1_sim_results.inputs, m1_sim_results.outputs, dt = 1))

        
        m.estimate_params(data, keys, method='CG', options={'maxiter': 1e-9, 'disp': False})
        m1.estimate_params(data_m1, keys, method='CG', options={'maxiter': 1e-9, 'disp': False})

        self.assertAlmostEqual(m.calc_error(simulated_results.times, simulated_results.inputs, simulated_results.outputs, dt = 1), 
                            m1.calc_error(m1_sim_results.times, m1_sim_results.inputs, m1_sim_results.outputs, dt = 1), 0)


# Reason for nan values appearing in the x value? or the z_obs value ( CHECK AGAIN U APE ) is because there is a lack of uuuh
# the xnS value is too small, it is less than one as defined in battery_electrochem.py output(). 
# That is because we are using the np.log() operator on a value that is negative. If that value was
# in between 0-1, then we would have a negative number, which may also be a problem (-inf, is returned whereas, we would have nan)
# finally, these values are not directly influenced by our change, however, this is probably being changed away from default parameters,
# from dx() parameter in prognostics_model.py